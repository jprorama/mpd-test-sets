{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score challenge submission\n",
    "\n",
    "This notebook produces scores for a custom challenge set.  It scores each tasks and provides a summary score (average).\n",
    "\n",
    "This is designed to help compare the quality of a solution against a challenge set under our control and demonstrate that the aicrowd score is consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import re\n",
    "import collections\n",
    "import os\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge=\"ex2-from-21k-train-with-pids\"\n",
    "submitdir=\"/home/jpr/projects/mpd-challenge-aicrowd/\"\n",
    "submissionfile=\"method-01-mympd-2nd-21k-2021-11-14.csv.gz\"\n",
    "submissions = [{\"tag\": \"vl6\", \"file\": submissionfile}]\n",
    "submissionfile=\"method-02-mympd-2nd-21k-2021-11-07.csv.gz\"\n",
    "submissions.append({\"tag\": \"hw\", \"file\": submissionfile})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the no holdouts challenge set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/{}/challenge_set_noholdout.json'.format(challenge),'r') as f:\n",
    "    data = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noholdout = pd.json_normalize(data,\"playlists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noholdout.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load challenge submission`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rectracks=pd.read_csv('{}/{}'.format(submitdir, submissionfile), header=None, skiprows=1, index_col=0, skipinitialspace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure the pids match those of the challenge set.  In the case of mympd the pid range starts at 200000 and goes up in sequence of noholdout data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rectracks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rectracks = rectracks.sort_values(by=0, axis='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rectracks.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score Challenge Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startpid=2000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R-precision is the fraction of correctly recommended tracks in the ground truth playlist as described on [the challenge site](https://www.aicrowd.com/challenges/spotify-million-playlist-dataset-challenge#evaluation)\n",
    "    \n",
    "\n",
    "Use the [r-precesion calculation from the hello_world metrics](https://github.com/jprorama/spotify_recSys_challenge_2018/blob/f33d82715190a20fdbc998c9ff709bcabd62a55e/utils/metrics.py#L26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_r_precision(answer, cand):\n",
    "    set_answer = set(answer)\n",
    "    r = len(set_answer&set(cand[:len(set_answer)])) / len(set_answer)\n",
    "    return r\n",
    "\n",
    "def get_ndcg(answer, cand):\n",
    "    cand_len = len(cand) \n",
    "    idcg=0\n",
    "    dcg=0\n",
    "    \n",
    "    #print(\"cand len {}\".format(cand_len))\n",
    "    #print(\"ans len {}\".format(len(answer)))\n",
    "    #print(\"cand: {}\".format(cand))\n",
    "    \n",
    "    for i in range(cand_len):\n",
    "        #print(\"i {}\".format(i))\n",
    "        #print(\"cand {}\".format(cand[i]))\n",
    "        if cand[i] in answer: \n",
    "            dcg += (1/math.log(i+1+1,2))\n",
    "\n",
    "    for i in range(len(set(answer))):\n",
    "        idcg += (1/math.log(i+1+1,2))\n",
    "    \n",
    "    return dcg/idcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "realtracks=pd.DataFrame()\n",
    "rprec_sum = 0.0\n",
    "ndcg_sum = 0.0\n",
    "rprec_match = 0\n",
    "ndcg_match = 0\n",
    "\n",
    "for i in range(len(noholdout)):\n",
    "    pid = startpid + i\n",
    "    #print(\"pid={}\".format(pid))\n",
    "    gttracks = [track[\"track_uri\"] for track in noholdout[noholdout[\"challenge_pid\"]==pid].tracks.to_list()[0]]\n",
    "    candtracks = rectracks.loc[pid].to_list()\n",
    "    rprec = get_r_precision(gttracks, candtracks)\n",
    "    rprec_sum = rprec_sum + rprec\n",
    "    if rprec > 0:\n",
    "        rprec_match += 1\n",
    "    ndcg = get_ndcg(gttracks, candtracks)\n",
    "    ndcg_sum = ndcg_sum + ndcg\n",
    "    if ndcg > 0:\n",
    "        ndcg_match += 1\n",
    "    \n",
    "print(\"rprec = {}\".format(rprec_sum/len(noholdout)))\n",
    "print(\"ndcg = {}\".format(ndcg_sum/len(noholdout)))\n",
    "print(\"rprec_match = {}\".format(rprec_match))\n",
    "print(\"ndcg_match = {}\".format(ndcg_match))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_set(answer, candidates):\n",
    "    rprec_sum = 0.0\n",
    "    ndcg_sum = 0.0\n",
    "    rprec_match = 0\n",
    "    ndcg_match = 0\n",
    "\n",
    "    scores = dict()\n",
    "\n",
    "    for pid in answer.challenge_pid:\n",
    "        #pid = startpid + i\n",
    "        #print(\"pid={}\".format(pid))\n",
    "        gttracks = [track[\"track_uri\"] for track in answer[answer[\"challenge_pid\"]==pid].tracks.to_list()[0]]\n",
    "        candtracks = candidates.loc[pid].to_list()\n",
    "        rprec = get_r_precision(gttracks, candtracks)\n",
    "        rprec_sum = rprec_sum + rprec\n",
    "        if rprec > 0:\n",
    "            rprec_match += 1\n",
    "        ndcg = get_ndcg(gttracks, candtracks)\n",
    "        ndcg_sum = ndcg_sum + ndcg\n",
    "        if ndcg > 0:\n",
    "            ndcg_match += 1\n",
    "        \n",
    "    scores[\"rprec\"] = rprec_sum/len(answer)\n",
    "    scores[\"rprec_match\"] = rprec_match\n",
    "    scores[\"ndcg\"] = ndcg_sum/len(answer)\n",
    "    scores[\"ndcg_match\"] = ndcg_match\n",
    "        \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rprec_sum = 0.0\n",
    "ndcg_sum = 0.0\n",
    "rprec_match = 0\n",
    "ndcg_match = 0\n",
    "\n",
    "scores = dict()\n",
    "\n",
    "for task in noholdout.task_name.drop_duplicates():\n",
    "\n",
    "    scores[task] = score_set(noholdout[noholdout[\"task_name\"]==task], rectracks)\n",
    "    \n",
    "    rprec_sum += scores[task][\"rprec\"]\n",
    "    rprec_match += scores[task][\"rprec_match\"]\n",
    "    ndcg_sum += scores[task][\"ndcg\"]\n",
    "    ndcg_match += scores[task][\"ndcg_match\"]\n",
    "    \n",
    "    \n",
    "    print(\"task: {}\".format(task))\n",
    "    print(\"rprec = {}\".format(scores[task][\"rprec\"]))\n",
    "    print(\"ndcg = {}\".format(scores[task][\"ndcg\"]))\n",
    "    print(\"rprec_match = {}\".format(scores[task][\"rprec_match\"]))\n",
    "    print(\"ndcg_match = {}\".format(scores[task][\"ndcg_match\"]))\n",
    "    print(\"\\n\")\n",
    "\n",
    "scores[\"total\"] = dict()\n",
    "scores[\"total\"][\"rprec\"] = rprec_sum/10\n",
    "scores[\"total\"][\"rprec_match\"] = rprec_match\n",
    "scores[\"total\"][\"ndcg\"] = ndcg_sum/10\n",
    "scores[\"total\"][\"ndcg_match\"] = ndcg_match\n",
    "\n",
    "print(\"task: {}\".format(\"total\"))\n",
    "print(\"rprec = {}\".format(scores[\"total\"][\"rprec\"]))\n",
    "print(\"ndcg = {}\".format(scores[\"total\"][\"ndcg\"]))\n",
    "print(\"rprec_match = {}\".format(scores[\"total\"][\"rprec_match\"]))\n",
    "print(\"ndcg_match = {}\".format(scores[\"total\"][\"ndcg_match\"]))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_setdf(answer, candidate):\n",
    "\n",
    "    rprec_sum = 0.0\n",
    "    ndcg_sum = 0.0\n",
    "    rprec_match = 0\n",
    "    ndcg_match = 0\n",
    "\n",
    "    results = answer.copy()\n",
    "\n",
    "    #results[\"candidates\"] = \n",
    "    results[\"rprec\"] = 0\n",
    "    results[\"ndcg\"] = 0\n",
    "\n",
    "    for pid in results.challenge_pid:\n",
    "        #pid = startpid + i\n",
    "        #print(\"pid={}\".format(pid))\n",
    "        gttracks = [track[\"track_uri\"] for track in results[results[\"challenge_pid\"]==pid].tracks.to_list()[0]]\n",
    "        candtracks = rectracks.loc[pid].to_list()\n",
    "        rprec = get_r_precision(gttracks, candtracks)\n",
    "        rprec_sum = rprec_sum + rprec\n",
    "        if rprec > 0:\n",
    "            rprec_match += 1\n",
    "        ndcg = get_ndcg(gttracks, candtracks)\n",
    "        ndcg_sum = ndcg_sum + ndcg\n",
    "        if ndcg > 0:\n",
    "            ndcg_match += 1\n",
    "\n",
    "        #results.at[results[\"challenge_pid\"]==pid, \"candidates\"] = candtracks\n",
    "        results.at[results[\"challenge_pid\"]==pid, \"rprec\"] = rprec\n",
    "        results.at[results[\"challenge_pid\"]==pid, \"ndcg\"] = ndcg\n",
    "        \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame()\n",
    "tmpdf = pd.DataFrame()\n",
    "\n",
    "for submission in submissions:\n",
    "    print(\"submission: {}\".format(submission))\n",
    "    print(\"file: {}\".format(submission[\"file\"]))\n",
    "    rectracks=pd.read_csv('{}/{}'.format(submitdir, submission[\"file\"]), header=None, skiprows=1, index_col=0, skipinitialspace=True)\n",
    "    tmpdf = score_setdf(noholdout, rectracks)\n",
    "    print(\"tmpdf len: {}\".format(len(tmpdf)))\n",
    "    tmpdf[\"tag\"]=submission[\"tag\"]\n",
    "    results=results.append(tmpdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.groupby([\"tag\", \"task_name\"]).rprec.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodup = results[results[\"tag\"]==\"vl6\"].drop_duplicates(subset=\"pid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodup = nodup.append(results[results[\"tag\"]==\"hw\"].drop_duplicates(subset=\"pid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodup.groupby([\"tag\", \"task_name\"]).rprec.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means= nodup.groupby([\"tag\", \"task_name\"]).rprec.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ax=means.hw.plot.bar(legend=True, color=\"red\", alpha=.5)\n",
    "means.vl6.plot.bar(ax=ax,legend=True, alpha=0.35)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Violin plot\n",
    "\n",
    "start with the routine used during the recsys18 analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_violin(df, title=\"Violin Plot\"):\n",
    "    #sns.set_style(\"white\") \n",
    "    sns.set(rc={'figure.figsize':(8,6)})\n",
    "    g = sns.violinplot(data=df, cut=0, orient='v', scale='width')\n",
    "    #g = sns.violinplot(x=df.iloc[,0], y=df.iloc[0,:], cut=0, scale='width')\n",
    "    g.set_title(title)\n",
    "    #g.set_xlabel(\"Subtask\")\n",
    "    g.set_ylabel(\"Score\")\n",
    "    g.set_xticklabels(g.get_xticklabels(), rotation=90)\n",
    "\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodup.groupby([\"tag\", \"task_name\"]).rprec.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't need the describe data because that will come from the violin plot.  Just need to use all the raw data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vl6desc = nodup[nodup[\"tag\"]==\"vl6\"][[\"task_name\", \"rprec\"]] #.groupby([\"task_name\"]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hwdesc = nodup[nodup[\"tag\"]==\"hw\"][[\"task_name\", \"rprec\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.violinplot(data=g, cut=0, orient='v', scale='width')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_violin(vl6desc[vl6desc[\"task_name\"]==\"rand_100_title\"], \"rand_100_title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_violin(hwdesc[hwdesc[\"task_name\"]==\"rand_100_title\"], \"rand_100_title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore side-by-side plots\n",
    "\n",
    "What to take all the tasks and see the teams side by side\n",
    "\n",
    "Create violin plots for each task. https://stackoverflow.com/a/47487445/8928529\n",
    "\n",
    "Basically loop through the tasks and plot on each subplot axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(10, 2, figsize=(20, 32), sharey='row')\n",
    "axes_cols = (axes.flatten()[::2], axes.flatten()[1::2])\n",
    "\n",
    "i=0\n",
    "for task in vl6desc.task_name.drop_duplicates():\n",
    "    \n",
    "    ax=axes_cols[0][i]\n",
    "    sns.violinplot(data=vl6desc[vl6desc[\"task_name\"]==task], cut=0, orient='v', scale='width', ax=ax)\n",
    "    ax.set_title('task = {}'.format(task), y=0.95)\n",
    "    ax=axes_cols[1][i]\n",
    "    sns.violinplot(data=hwdesc[hwdesc[\"task_name\"]==task], cut=0, orient='v', scale='width', ax=ax)\n",
    "    ax.set_title('task = {}'.format(task), y=0.95)\n",
    "    \n",
    "    \n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore split plot\n",
    "\n",
    "This lets me see the data paired directly and allows easier visual comparison of differences.\n",
    "\n",
    "The test rand_100_title task shows clear differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = nodup[[\"tag\",\"task_name\", \"rprec\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.violinplot(data=data[data[\"task_name\"]==\"rand_100_title\"], cut=0, orient='v', scale='width',\n",
    "               x=\"task_name\", y=\"rprec\",\n",
    "               hue=\"tag\",\n",
    "               split=True, inner=\"quart\")\n",
    "\n",
    "#sns.despine(left=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rprec_data = nodup[[\"tag\",\"task_name\", \"rprec\"]]\n",
    "ndcg_data  = nodup[[\"tag\",\"task_name\", \"ndcg\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(10, 2, figsize=(20, 32), sharey='row')\n",
    "axes_cols = (axes.flatten()[::2], axes.flatten()[1::2])\n",
    "\n",
    "i=0\n",
    "for task in vl6desc.task_name.drop_duplicates():\n",
    "    \n",
    "    ax=axes_cols[0][i]\n",
    "    sns.violinplot(data=rprec_data[rprec_data[\"task_name\"]==task], cut=0, orient='v', scale='width',\n",
    "               x=\"task_name\", y=\"rprec\",\n",
    "               hue=\"tag\",\n",
    "               split=True, inner=\"quart\", ax=ax)\n",
    "    ax.set_title('task = {}'.format(task), y=0.95)\n",
    "\n",
    "    ax=axes_cols[1][i]\n",
    "    sns.violinplot(data=ndcg_data[ndcg_data[\"task_name\"]==task], cut=0, orient='v', scale='width',\n",
    "               x=\"task_name\", y=\"ndcg\",\n",
    "               hue=\"tag\",\n",
    "               split=True, inner=\"quart\", ax=ax)\n",
    "    ax.set_title('task = {}'.format(task), y=0.95)\n",
    "    \n",
    "    \n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the rprec and ndcg data plotted side by side it is clear that the rand_100_title was much more effectively solved by vl6.\n",
    "\n",
    "The second most effective was first_1_title where vl6 had higher mean.\n",
    "\n",
    "However the the title_only solution of hw is clearly better.\n",
    "\n",
    "All the rest of the tasks had nearly identical means and distributions with the hw solution having slightly higher ndcg in those tasks.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
