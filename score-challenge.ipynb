{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score challenge submission\n",
    "\n",
    "This notebook produces scores for a custom challenge set.  It scores each tasks and provides a summary score (average).\n",
    "\n",
    "This is designed to help compare the quality of a solution against a challenge set under our control and demonstrate that the aicrowd score is consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import re\n",
    "import collections\n",
    "import os\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge=\"ex2-from-21k-train-with-pids\"\n",
    "submitdir=\"/home/jpr/projects/mpd-challenge-aicrowd/\"\n",
    "submissionfile=\"method-01-mympd-2nd-21k-2021-11-14.csv.gz\"\n",
    "submissions = [{\"tag\": \"vl6\", \"file\": submissionfile}]\n",
    "submissionfile=\"method-02-mympd-2nd-21k-2021-11-07.csv.gz\"\n",
    "submissions.append({\"tag\": \"hw\", \"file\": submissionfile})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the no holdouts challenge set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/{}/challenge_set_noholdout.json'.format(challenge),'r') as f:\n",
    "    data = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noholdout = pd.json_normalize(data,\"playlists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noholdout.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load challenge submission`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rectracks=pd.read_csv('{}/{}'.format(submitdir, submissionfile), header=None, skiprows=1, index_col=0, skipinitialspace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure the pids match those of the challenge set.  In the case of mympd the pid range starts at 200000 and goes up in sequence of noholdout data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rectracks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rectracks = rectracks.sort_values(by=0, axis='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rectracks.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score Challenge Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startpid=2000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R-precision is the fraction of correctly recommended tracks in the ground truth playlist as described on [the challenge site](https://www.aicrowd.com/challenges/spotify-million-playlist-dataset-challenge#evaluation)\n",
    "    \n",
    "\n",
    "Use the [r-precesion calculation from the hello_world metrics](https://github.com/jprorama/spotify_recSys_challenge_2018/blob/f33d82715190a20fdbc998c9ff709bcabd62a55e/utils/metrics.py#L26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_r_precision(answer, cand):\n",
    "    set_answer = set(answer)\n",
    "    r = len(set_answer&set(cand[:len(set_answer)])) / len(set_answer)\n",
    "    return r\n",
    "\n",
    "def get_ndcg(answer, cand):\n",
    "    cand_len = len(cand) \n",
    "    idcg=0\n",
    "    dcg=0\n",
    "    \n",
    "    #print(\"cand len {}\".format(cand_len))\n",
    "    #print(\"ans len {}\".format(len(answer)))\n",
    "    #print(\"cand: {}\".format(cand))\n",
    "    \n",
    "    for i in range(cand_len):\n",
    "        #print(\"i {}\".format(i))\n",
    "        #print(\"cand {}\".format(cand[i]))\n",
    "        if cand[i] in answer: \n",
    "            dcg += (1/math.log(i+1+1,2))\n",
    "\n",
    "    for i in range(len(set(answer))):\n",
    "        idcg += (1/math.log(i+1+1,2))\n",
    "    \n",
    "    return dcg/idcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "realtracks=pd.DataFrame()\n",
    "rprec_sum = 0.0\n",
    "ndcg_sum = 0.0\n",
    "rprec_match = 0\n",
    "ndcg_match = 0\n",
    "\n",
    "for i in range(len(noholdout)):\n",
    "    pid = startpid + i\n",
    "    #print(\"pid={}\".format(pid))\n",
    "    gttracks = [track[\"track_uri\"] for track in noholdout[noholdout[\"challenge_pid\"]==pid].tracks.to_list()[0]]\n",
    "    candtracks = rectracks.loc[pid].to_list()\n",
    "    rprec = get_r_precision(gttracks, candtracks)\n",
    "    rprec_sum = rprec_sum + rprec\n",
    "    if rprec > 0:\n",
    "        rprec_match += 1\n",
    "    ndcg = get_ndcg(gttracks, candtracks)\n",
    "    ndcg_sum = ndcg_sum + ndcg\n",
    "    if ndcg > 0:\n",
    "        ndcg_match += 1\n",
    "    \n",
    "print(\"rprec = {}\".format(rprec_sum/len(noholdout)))\n",
    "print(\"ndcg = {}\".format(ndcg_sum/len(noholdout)))\n",
    "print(\"rprec_match = {}\".format(rprec_match))\n",
    "print(\"ndcg_match = {}\".format(ndcg_match))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_set(answer, candidates):\n",
    "    rprec_sum = 0.0\n",
    "    ndcg_sum = 0.0\n",
    "    rprec_match = 0\n",
    "    ndcg_match = 0\n",
    "\n",
    "    scores = dict()\n",
    "\n",
    "    for pid in answer.challenge_pid:\n",
    "        #pid = startpid + i\n",
    "        #print(\"pid={}\".format(pid))\n",
    "        gttracks = [track[\"track_uri\"] for track in answer[answer[\"challenge_pid\"]==pid].tracks.to_list()[0]]\n",
    "        candtracks = candidates.loc[pid].to_list()\n",
    "        rprec = get_r_precision(gttracks, candtracks)\n",
    "        rprec_sum = rprec_sum + rprec\n",
    "        if rprec > 0:\n",
    "            rprec_match += 1\n",
    "        ndcg = get_ndcg(gttracks, candtracks)\n",
    "        ndcg_sum = ndcg_sum + ndcg\n",
    "        if ndcg > 0:\n",
    "            ndcg_match += 1\n",
    "        \n",
    "    scores[\"rprec\"] = rprec_sum/len(answer)\n",
    "    scores[\"rprec_match\"] = rprec_match\n",
    "    scores[\"ndcg\"] = ndcg_sum/len(answer)\n",
    "    scores[\"ndcg_match\"] = ndcg_match\n",
    "        \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rprec_sum = 0.0\n",
    "ndcg_sum = 0.0\n",
    "rprec_match = 0\n",
    "ndcg_match = 0\n",
    "\n",
    "scores = dict()\n",
    "\n",
    "for task in noholdout.task_name.drop_duplicates():\n",
    "\n",
    "    scores[task] = score_set(noholdout[noholdout[\"task_name\"]==task], rectracks)\n",
    "    \n",
    "    rprec_sum += scores[task][\"rprec\"]\n",
    "    rprec_match += scores[task][\"rprec_match\"]\n",
    "    ndcg_sum += scores[task][\"ndcg\"]\n",
    "    ndcg_match += scores[task][\"ndcg_match\"]\n",
    "    \n",
    "    \n",
    "    print(\"task: {}\".format(task))\n",
    "    print(\"rprec = {}\".format(scores[task][\"rprec\"]))\n",
    "    print(\"ndcg = {}\".format(scores[task][\"ndcg\"]))\n",
    "    print(\"rprec_match = {}\".format(scores[task][\"rprec_match\"]))\n",
    "    print(\"ndcg_match = {}\".format(scores[task][\"ndcg_match\"]))\n",
    "    print(\"\\n\")\n",
    "\n",
    "scores[\"total\"] = dict()\n",
    "scores[\"total\"][\"rprec\"] = rprec_sum/10\n",
    "scores[\"total\"][\"rprec_match\"] = rprec_match\n",
    "scores[\"total\"][\"ndcg\"] = ndcg_sum/10\n",
    "scores[\"total\"][\"ndcg_match\"] = ndcg_match\n",
    "\n",
    "print(\"task: {}\".format(\"total\"))\n",
    "print(\"rprec = {}\".format(scores[\"total\"][\"rprec\"]))\n",
    "print(\"ndcg = {}\".format(scores[\"total\"][\"ndcg\"]))\n",
    "print(\"rprec_match = {}\".format(scores[\"total\"][\"rprec_match\"]))\n",
    "print(\"ndcg_match = {}\".format(scores[\"total\"][\"ndcg_match\"]))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_setdf(answer, candidate):\n",
    "\n",
    "    rprec_sum = 0.0\n",
    "    ndcg_sum = 0.0\n",
    "    rprec_match = 0\n",
    "    ndcg_match = 0\n",
    "\n",
    "    results = answer.copy()\n",
    "\n",
    "    #results[\"candidates\"] = \n",
    "    results[\"rprec\"] = 0\n",
    "    results[\"ndcg\"] = 0\n",
    "\n",
    "    for pid in results.challenge_pid:\n",
    "        #pid = startpid + i\n",
    "        #print(\"pid={}\".format(pid))\n",
    "        gttracks = [track[\"track_uri\"] for track in results[results[\"challenge_pid\"]==pid].tracks.to_list()[0]]\n",
    "        candtracks = rectracks.loc[pid].to_list()\n",
    "        rprec = get_r_precision(gttracks, candtracks)\n",
    "        rprec_sum = rprec_sum + rprec\n",
    "        if rprec > 0:\n",
    "            rprec_match += 1\n",
    "        ndcg = get_ndcg(gttracks, candtracks)\n",
    "        ndcg_sum = ndcg_sum + ndcg\n",
    "        if ndcg > 0:\n",
    "            ndcg_match += 1\n",
    "\n",
    "        #results.at[results[\"challenge_pid\"]==pid, \"candidates\"] = candtracks\n",
    "        results.at[results[\"challenge_pid\"]==pid, \"rprec\"] = rprec\n",
    "        results.at[results[\"challenge_pid\"]==pid, \"ndcg\"] = ndcg\n",
    "        \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame()\n",
    "tmpdf = pd.DataFrame()\n",
    "\n",
    "for submission in submissions:\n",
    "    print(\"submission: {}\".format(submission))\n",
    "    print(\"file: {}\".format(submission[\"file\"]))\n",
    "    rectracks=pd.read_csv('{}/{}'.format(submitdir, submission[\"file\"]), header=None, skiprows=1, index_col=0, skipinitialspace=True)\n",
    "    tmpdf = score_setdf(noholdout, rectracks)\n",
    "    print(\"tmpdf len: {}\".format(len(tmpdf)))\n",
    "    tmpdf[\"tag\"]=submission[\"tag\"]\n",
    "    results=results.append(tmpdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.groupby([\"tag\", \"task_name\"]).rprec.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodup = results[results[\"tag\"]==\"vl6\"].drop_duplicates(subset=\"pid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodup = nodup.append(results[results[\"tag\"]==\"hw\"].drop_duplicates(subset=\"pid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodup.groupby([\"tag\", \"task_name\"]).rprec.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means= nodup.groupby([\"tag\", \"task_name\"]).rprec.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ax=means.hw.plot.bar(legend=True, color=\"red\", alpha=.5)\n",
    "means.vl6.plot.bar(ax=ax,legend=True, alpha=0.35)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
