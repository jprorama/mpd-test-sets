{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score challenge submission\n",
    "\n",
    "This notebook produces scores for a custom challenge set.  It scores each tasks and provides a summary score (average).\n",
    "\n",
    "This is designed to help compare the quality of a solution against a challenge set under our control and demonstrate that the aicrowd score is consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import re\n",
    "import collections\n",
    "import os\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experimentsdir=\"/home/jpr/projects/mpd-test-sets/results/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments=list()\n",
    "#experiments.append(\"/home/jpr/projects/mpd-test-sets/results/u2u-optimized-scaled-complete\")\n",
    "#experiments.append(\"/home/jpr/projects/mpd-test-sets/results/u2u-optimized-scaled-complete-redux\")\n",
    "#experiments.append(\"/home/jpr/projects/mpd-test-sets/results/knn-u2u-unified-sklearn-sim-train-only\")\n",
    "experiments.append(\"/home/jpr/projects/mpd-test-sets/results/knn-u2u-unified-cos-similarities2_1\")\n",
    "#experiments.append(\"/home/jpr/projects/mpd-test-sets/results/knn-u2u-unified-cos-similarities2_2\")\n",
    "experiments.append(\"/home/jpr/projects/mpd-test-sets/results/knn-u2u-unified-cos-similarities3\")\n",
    "experiments.append(\"/home/jpr/projects/mpd-test-sets/results/knn-u2u-unified-tfidf-scaling-sim\")\n",
    "#experiments.append(\"/home/jpr/projects/mpd-test-sets/results/i2i-optimized-scaled-complete\")\n",
    "#experiments.append(\"/home/jpr/projects/mpd-test-sets/results/knn-i2i-unified\")\n",
    "#experiments.append(\"/home/jpr/projects/mpd-test-sets/results/knn-i2i-original\")\n",
    "#experiments.append(\"/home/jpr/projects/mpd-test-sets/results/knn-i2i-original-train-only\")\n",
    "experiments.append(\"/home/jpr/projects/mpd-test-sets/results/knn-i2i-unified-sklearn-sim-train-only\")\n",
    "#experiments.append(\"/home/jpr/projects/mpd-test-sets/results/mfals-optimized-scaled\")\n",
    "experiments.append(\"/home/jpr/projects/mpd-test-sets/results/mfals-allmpb-wo-transpose-mpd3\")\n",
    "experiments.append(\"/home/jpr/projects/mpd-test-sets/results/vl6-mympd-full\")\n",
    "#experiments.append(\"/home/jpr/projects/mpd-test-sets/results/mfals-manual-results-2022-01-02\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submissions=[]\n",
    "\n",
    "for submitdir in experiments:\n",
    "    with os.scandir(submitdir) as entries:\n",
    "        for entry in entries:\n",
    "            if entry.name.endswith(\".gz\"):\n",
    "                print(entry.name)\n",
    "                method, challenge, trainset, ignore = entry.name.split(\"_\", 3)\n",
    "                ignore, method = method.split(\"-\", 1)\n",
    "                trainset, size = trainset.rsplit(\"-\", 1)\n",
    "                size = size.split('k')[0]\n",
    "                ignore, tag = submitdir.rsplit(\"/\", 1)\n",
    "\n",
    "                submissions.append({\"method\": method, \n",
    "                        \"challenge\": challenge, \n",
    "                        \"trainset\": trainset, \n",
    "                        \"trainsize\": size, \n",
    "                        \"tag\": tag, \n",
    "                        \"dir\": submitdir + \"/\",\n",
    "                        \"file\": entry.name})\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submissions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the no holdouts challenge set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_challenges(submissions):\n",
    "    \n",
    "    challenges=set()\n",
    "\n",
    "    for exp in submissions:\n",
    "        challenges.add(exp[\"challenge\"])\n",
    "        \n",
    "    return list(challenges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorder_holdout(noholdout):\n",
    "    tmpdf=noholdout[0:6000]\n",
    "    tmpdf=tmpdf.append(noholdout[7000:8000])\n",
    "    tmpdf=tmpdf.append(noholdout[6000:7000])\n",
    "    tmpdf=tmpdf.append(noholdout[8000:10000])\n",
    "    return tmpdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_answers(answers_list):\n",
    "\n",
    "    answers = dict()\n",
    "    \n",
    "    for answer in answers_list:\n",
    "        with open('data/{}/challenge_set_noholdout.json'.format(answer),'r') as f:\n",
    "            data = json.loads(f.read())\n",
    "\n",
    "        data = pd.json_normalize(data,\"playlists\")\n",
    "        data = reorder_holdout(data)\n",
    "        answers[answer] = data\n",
    "        \n",
    "    return answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "challenges = get_challenges(submissions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "challenges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = load_answers(challenges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load challenge submission`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score Challenge Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R-precision is the fraction of correctly recommended tracks in the ground truth playlist as described on [the challenge site](https://www.aicrowd.com/challenges/spotify-million-playlist-dataset-challenge#evaluation)\n",
    "    \n",
    "\n",
    "Use the [r-precesion calculation from the hello_world metrics](https://github.com/jprorama/spotify_recSys_challenge_2018/blob/f33d82715190a20fdbc998c9ff709bcabd62a55e/utils/metrics.py#L26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_r_precision(answer, cand):\n",
    "    set_answer = set(answer)\n",
    "    r = len(set_answer&set(cand[:len(set_answer)])) / len(set_answer)\n",
    "    return r\n",
    "\n",
    "def get_ndcg(answer, cand):\n",
    "    cand_len = len(cand) \n",
    "    idcg=0\n",
    "    dcg=0\n",
    "    \n",
    "    #print(\"cand len {}\".format(cand_len))\n",
    "    #print(\"ans len {}\".format(len(answer)))\n",
    "    #print(\"cand: {}\".format(cand))\n",
    "    \n",
    "    for i in range(cand_len):\n",
    "        #print(\"i {}\".format(i))\n",
    "        #print(\"cand {}\".format(cand[i]))\n",
    "        if cand[i] in answer: \n",
    "            dcg += (1/math.log(i+1+1,2))\n",
    "\n",
    "    for i in range(len(set(answer))):\n",
    "        idcg += (1/math.log(i+1+1,2))\n",
    "    \n",
    "    return dcg/idcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_set(answer, candidates):\n",
    "    rprec_sum = 0.0\n",
    "    ndcg_sum = 0.0\n",
    "    rprec_match = 0\n",
    "    ndcg_match = 0\n",
    "\n",
    "    scores = dict()\n",
    "\n",
    "    for pid in answer.challenge_pid:\n",
    "        #pid = startpid + i\n",
    "        #print(\"pid={}\".format(pid))\n",
    "        gttracks = [track[\"track_uri\"] for track in answer[answer[\"challenge_pid\"]==pid].tracks.to_list()[0]]\n",
    "        candtracks = candidates.loc[pid].to_list()\n",
    "        rprec = get_r_precision(gttracks, candtracks)\n",
    "        rprec_sum = rprec_sum + rprec\n",
    "        if rprec > 0:\n",
    "            rprec_match += 1\n",
    "        ndcg = get_ndcg(gttracks, candtracks)\n",
    "        ndcg_sum = ndcg_sum + ndcg\n",
    "        if ndcg > 0:\n",
    "            ndcg_match += 1\n",
    "        \n",
    "    scores[\"rprec\"] = rprec_sum/len(answer)\n",
    "    scores[\"rprec_match\"] = rprec_match\n",
    "    scores[\"ndcg\"] = ndcg_sum/len(answer)\n",
    "    scores[\"ndcg_match\"] = ndcg_match\n",
    "        \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_setdf(answer, candidate):\n",
    "\n",
    "    rprec_sum = 0.0\n",
    "    ndcg_sum = 0.0\n",
    "    rprec_match = 0\n",
    "    ndcg_match = 0\n",
    "\n",
    "    results = answer.copy()\n",
    "\n",
    "    #results[\"candidates\"] = \n",
    "    results[\"rprec\"] = 0\n",
    "    #results[\"rprec_alt\"] = 0\n",
    "    results[\"ndcg\"] = 0\n",
    "\n",
    "    for pid in results.challenge_pid:\n",
    "        #pid = startpid + i\n",
    "        #print(\"pid={}\".format(pid))\n",
    "        gttracks = [track[\"track_uri\"] for track in results[results[\"challenge_pid\"]==pid].tracks.to_list()[0]]\n",
    "        candtracks = rectracks.loc[pid].to_list()\n",
    "        rprec = get_r_precision(gttracks, candtracks)\n",
    "        #if not results[results[\"challenge_pid\"]==pid].random:\n",
    "        #    num_samples=results[results[\"challenge_pid\"]==pid].num_samples\n",
    "        #    rprec_alt =  get_r_precision(gttracks[num_samples:], candtracks)\n",
    "        rprec_sum = rprec_sum + rprec\n",
    "        if rprec > 0:\n",
    "            rprec_match += 1\n",
    "        ndcg = get_ndcg(gttracks, candtracks)\n",
    "        ndcg_sum = ndcg_sum + ndcg\n",
    "        if ndcg > 0:\n",
    "            ndcg_match += 1\n",
    "\n",
    "        #results.at[results[\"challenge_pid\"]==pid, \"candidates\"] = candtracks\n",
    "        results.at[results[\"challenge_pid\"]==pid, \"rprec\"] = rprec\n",
    "        #results.at[results[\"challenge_pid\"]==pid, \"rprec_alt\"] = rprec_alt\n",
    "        results.at[results[\"challenge_pid\"]==pid, \"ndcg\"] = ndcg\n",
    "        \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "rlist=[]\n",
    "results = pd.DataFrame()\n",
    "tmpdf = pd.DataFrame()\n",
    "\n",
    "for submission in submissions:\n",
    "    print(\"submission: {}\".format(submission))\n",
    "    print(\"file: {}\".format(submission[\"file\"]))\n",
    "    \n",
    "    cachefile=submission[\"dir\"]+\"cache/scored-pickle-\"+submission[\"file\"]\n",
    "\n",
    "    start_time = time.time()    \n",
    "    if (os.path.isfile(cachefile)):\n",
    "        tmpdf = pd.read_pickle(cachefile)\n",
    "        print(\"score read cache: {} sec\".format(time.time()-start_time))\n",
    "        \n",
    "        tmpdf[\"trainsize\"]=pd.to_numeric(submission[\"trainsize\"])\n",
    "        tmpdf[\"tag\"]=submission[\"tag\"]\n",
    "        tmpdf[\"method\"]=submission[\"method\"]\n",
    " \n",
    "    else:\n",
    "        start_time = time.time()\n",
    "        rectracks=pd.read_csv('{}/{}'.format(submission[\"dir\"], submission[\"file\"]), header=None, skiprows=1, index_col=0, skipinitialspace=True)\n",
    "        print(\"load time: {} sec\".format(time.time()-start_time))\n",
    "\n",
    "        #if (\"full\" in submission[\"tag\"]):\n",
    "        #if (submission[\"challenge\"]==\"mympd-full\"):\n",
    "        #    print(\"scored with noholdout2\")\n",
    "        #    tmpdf = score_setdf(noholdout2, rectracks)\n",
    "        #else:    \n",
    "        #    print(\"scored with noholdout\")\n",
    "        #    tmpdf = score_setdf(noholdout, rectracks)\n",
    "            \n",
    "        print(\"scored with: {}\".format(submission[\"challenge\"]))\n",
    "        tmpdf = score_setdf(answers[submission[\"challenge\"]], rectracks)\n",
    "        print(\"score time: {} sec\".format(time.time()-start_time))\n",
    "\n",
    "        tmpdf[\"trainsize\"]=pd.to_numeric(submission[\"trainsize\"])\n",
    "        tmpdf[\"tag\"]=submission[\"tag\"]\n",
    "        tmpdf[\"method\"]=submission[\"method\"]\n",
    "    \n",
    "        tmpdf.to_pickle(cachefile)\n",
    "        \n",
    "\n",
    "    print(\"tmpdf len: {}\".format(len(tmpdf)))\n",
    "    #tmpdf[\"trainsizek\"]=submission[\"trainsizek\"]\n",
    "    #tmpdf[\"tag\"]=submission[\"tag\"]\n",
    "    #tmpdf[\"method\"]=submission[\"method\"]\n",
    "    rlist.append(tmpdf.drop(columns=[\"tracks\"]))\n",
    "    \n",
    "results=pd.concat(rlist)\n",
    "results.reset_index(drop=True, inplace=True)\n",
    "del rlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize total peformance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[\"tag\"].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[[\"tag\", \"trainsize\"]].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[\"trainsize\"]=pd.to_numeric(results[\"trainsize\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rprec_scores=results.groupby([\"tag\", \"trainsize\"]).rprec.mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rprec_scores.sort_values(\"trainsize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rprec_scores[rprec_scores[\"trainsize\"]==700].sort_values(\"rprec\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r100=rprec_scores[rprec_scores[\"trainsize\"]==\"100\"].sort_values(\"rprec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r100.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rprec_scores.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.groupby([\"tag\", \"trainsize\"]).ndcg.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = results.groupby([\"tag\", \"method\", \"task_name\", \"trainsize\"], sort=False).mean().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "means.rename(columns={\"trainsizek\": \"trainsize\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means[\"trainsize\"]=pd.to_numeric(means[\"trainsize\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rprecs=means.groupby([\"tag\", \"trainsize\"]).rprec.mean().reset_index()\n",
    "ndcgs=means.groupby([\"tag\", \"trainsize\"]).ndcg.mean().reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rprecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "means.groupby([\"tag\", \"trainsize\"]).mean([\"rprec\", \"ndcg\"]).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compete=pd.merge(rprecs, ndcgs, how=\"outer\", on=[\"tag\", \"trainsize\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance of vl6 on aicrowd mpd challenge with f=200 and a=0.001.\n",
    "Manually entered from web results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vl6_mpd = pd.DataFrame({\n",
    "    \"trainsize\": [700, 500, 400, 300, 200, 100, 80, 40, 20],\n",
    "    \"rprec\": [0.21442484143065002, 0.21405716708527278, 0.2125500586026767, 0.2110791021070401, 0.2086606922904522, 0.19836474965963136, 0.19577806512432588, 0.1860676190999628, 0.16644686115093993],\n",
    "    \"ndcg\": [0.38145766548782994, 0.37883582620961503, 0.37649177888485413, 0.3740181704034522, 0.3689999024579872, 0.3503513287427269, 0.3452777465178121, 0.3270989490096962, 0.2939440975370557]\n",
    "                       \n",
    "})\n",
    "vl6_mpd[\"tag\"] = \"vl6-mpd\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vl6_mpd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance of vl6 on aicrowd mpd challenge with f=200 and a=0.001.\n",
    "Manually entered from web results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfals_mpd = pd.DataFrame({\n",
    "    \"trainsize\": [600, 500, 400, 300, 200, 100, 80, 60, 40, 20],\n",
    "    \"rprec\": [0.1882107724508686, 0.18809184200786833, 0.18835980226790114, 0.1880982511047418, 0.1878392459596271, 0.18598005010706134, 0.18558435129023018, 0.18431149868643149, 0.18272959865796876, 0.17787711395877562],\n",
    "    \"ndcg\": [0.3351378652838204, 0.3346447814399467, 0.3346665743142349, 0.33456525706787454, 0.33340858421290287, 0.33022997245582436, 0.3287635775126507, 0.32575433842270896, 0.3218402305031761, 0.3118166024554931]\n",
    "                       \n",
    "})\n",
    "mfals_mpd[\"tag\"] = \"mfals-mpd\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfals_mpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u2u_mpd_sim21 = pd.DataFrame({\n",
    "    \"trainsize\": [700, 600, 300, 200, 40, 20],\n",
    "    \"rprec\": [0.1842170921417326, 0.18396194763810977, 0.1832626778689676, 0.18256715453129624, 0.17621346517051084, 0.17094090669426182],\n",
    "    \"ndcg\": [0.3310907819918829, 0.33058729397194647, 0.32885352469852763, 0.32727258664454334, 0.31282312022279773, 0.30092662995758007]\n",
    "                       \n",
    "})\n",
    "u2u_mpd_sim21[\"tag\"] = \"u2u-unified-sim2_1-mpd\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u2u_mpd_sim21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i2i_mpd = pd.DataFrame({\n",
    "    \"trainsize\": [20, 10],\n",
    "    \"rprec\": [0.1530146983297283, 0.14086699336844719],\n",
    "    \"ndcg\": [0.2735167051481905, 0.25238909933343595]\n",
    "                       \n",
    "})\n",
    "i2i_mpd[\"tag\"] = \"i2i-unified-mpd\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compete=pd.concat([compete, vl6_mpd]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compete=pd.concat([compete, mfals_mpd]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compete=pd.concat([compete, u2u_mpd_sim21]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compete=pd.concat([compete, i2i_mpd]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process the aicrowd-results_ prefixed files to get the results from the autosubmission pipeline.\n",
    "\n",
    "After parsing the results merge them with the manually entered values above so they can contribute to the plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = Path(experimentsdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aicrowd_results = []\n",
    "\n",
    "for mpdexp in exp.glob(\"*-mpd\"):\n",
    "    mpd = Path(mpdexp)\n",
    "    expname = mpdexp.name.split(\"/\", -1)\n",
    "    expname = expname[0]\n",
    "    #print(expname)\n",
    "    for result in mpd.glob(\"aicrowd-result_*\"):\n",
    "        #print(result.name.rsplit(\"/\", -1))\n",
    "        parts = result.name.split(\"_\")\n",
    "        with open(result) as r:\n",
    "            curtest = {}\n",
    "            curtest[\"tag\"] = expname\n",
    "            trainsize = parts[3].rsplit(\"-\", 1)[1].rstrip(\"k\")\n",
    "            curtest[\"trainsize\"] = trainsize\n",
    "            for line in r.readlines():\n",
    "                test, score = line.split(\":\")\n",
    "                curtest[test] = score.strip()\n",
    "            aicrowd_results.append(curtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aicrowd_results=pd.DataFrame(aicrowd_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aicrowd_results[\"trainsize\"] = aicrowd_results[\"trainsize\"].astype(\"int32\")\n",
    "aicrowd_results[\"rprec\"] = aicrowd_results[\"rprec\"].astype(\"float64\")\n",
    "aicrowd_results[\"ndcg\"] = aicrowd_results[\"ndcg\"].astype(\"float64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aicrowd_results[[\"tag\", \"trainsize\",\"rprec\", \"ndcg\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compete2=pd.merge(compete, aicrowd_results[[\"tag\", \"trainsize\",\"rprec\", \"ndcg\"]], how=\"outer\", on=[\"tag\", \"trainsize\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compete2[\"rprec\"] = compete2[\"rprec_x\"].combine_first(compete2['rprec_y'])\n",
    "compete2[\"ndcg\"] = compete2[\"ndcg_x\"].combine_first(compete2['ndcg_y'])\n",
    "compete2 = compete2.drop(columns=[\"rprec_x\", \"rprec_y\", \"ndcg_x\", \"ndcg_y\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compete2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compete=compete2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x=\"trainsize\", y=\"rprec\", data=compete, hue=\"tag\")\n",
    "#ignore=plt.title(\"R-prec scaling mympd and mpd\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=\"trainsize\", y=\"rprec\", data=compete, hue=\"tag\")\n",
    "#ignore=plt.title(\"R-prec scaling mympd and mpd\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x=\"trainsize\", y=\"ndcg\", data=compete, hue=\"tag\")\n",
    "#ignore=plt.title(\"R-prec scaling mympd and mpd\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x = \"task_name\",       # x variable name\n",
    "            y = \"rprec\",       # y variable name\n",
    "            hue = \"tag\",  # group variable name\n",
    "            data = means,     # dataframe to plot\n",
    "            kind = \"bar\",\n",
    "            aspect = 2)\n",
    "ignore=plt.xticks(rotation=45)\n",
    "ignore=plt.title(\"R-Precision Compared\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x = \"task_name\",       # x variable name\n",
    "            y = \"ndcg\",       # y variable name\n",
    "            hue = \"tag\",  # group variable name\n",
    "            data = means,     # dataframe to plot\n",
    "            kind = \"bar\",\n",
    "            aspect = 2)\n",
    "ignore=plt.xticks(rotation=45)\n",
    "ignore=plt.title(\"NDCG Compared\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#g = sns.FacetGrid(means, col=\"task_name\", hue=\"method\", col_wrap=4, sharex=False, margin_titles=True, \n",
    "#                  height=6, aspect=.75,\n",
    "#                  col_order=[\"first_5_title\", \"first_5_wo_title\", \"first_10_title\", \"first_10_wo_title\",\n",
    "#                            \"first_25_title\", \"first_100_title\", \"rand_25_title\", \"rand_100_title\",\n",
    "#                            \"first_1_title\", \"title_only\"])\n",
    "#g.map(sns.lineplot, \"trainsize\", \"rprec\", marker=\"o\", legend=\"full\", linewidth = 4)\n",
    "g = sns.lineplot(data=means, x=\"trainsize\", y=\"rprec\", hue=\"tag\", marker=\"o\", legend=\"full\", linewidth = 4)\n",
    "#g.add_legend()\n",
    "#g.set_titles(col_template=\"{col_name}\")\n",
    "#g.set(xscale=\"log\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Panel of Methods with Spread\n",
    "\n",
    "Separating out the range plots in a facetgrid lets me see the overall performance of each of the measured methods so far.\n",
    "Vl6 definitely does a better job of lifting more playlists out of the \"poverty\" of poor recommendations.\n",
    "You can look at the bottom and top boundaries of the curves easily.\n",
    "There are several that have an interesting decrease in the lower bounary at a larger training set.\n",
    "\n",
    "This is a very helpful plot.\n",
    "Tts makes it clear that vl6 does produce a tighter bound on the recommendations (higher min and max)\n",
    "Their average perf on the 700k is almost better than the best possible on all the other methods.\n",
    "The i2i and u2u_sim21 and mfals are all competative but have more lower preforming recommendations.\n",
    "This weights down their average.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(means, col=\"tag\", hue=\"tag\", col_wrap=3, margin_titles=False, \n",
    "                  height=6, aspect=.75)\n",
    "g.map(sns.lineplot, \"trainsize\", \"rprec\", marker=\"o\",  linewidth = 4)\n",
    "#g.add_legend()\n",
    "#g.set_titles(col_template=\"{col_name}\")\n",
    "#g.set(xscale=\"log\")\n",
    "#plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(means[means[\"task_name\"]!=\"title_only\"], col=\"tag\", hue=\"tag\", col_wrap=3, margin_titles=False, \n",
    "                  height=6, aspect=.75)\n",
    "g.map(sns.lineplot, \"trainsize\", \"rprec\", marker=\"o\",  linewidth = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(means[means[\"with_title\"]==0], col=\"tag\", hue=\"tag\", col_wrap=3, margin_titles=False, \n",
    "                  height=6, aspect=.75)\n",
    "g.map(sns.lineplot, \"trainsize\", \"rprec\", marker=\"o\",  linewidth = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(means, col=\"task_name\", hue=\"tag\", col_wrap=4, sharex=False, margin_titles=True, \n",
    "                  height=6, aspect=.75,\n",
    "                  col_order=[\"first_5_title\", \"first_5_wo_title\", \"first_10_title\", \"first_10_wo_title\",\n",
    "                            \"first_25_title\", \"first_100_title\", \"rand_25_title\", \"rand_100_title\",\n",
    "                            \"first_1_title\", \"title_only\"])\n",
    "g.map(sns.lineplot, \"trainsize\", \"rprec\", marker=\"o\", legend=\"full\", linewidth = 4)\n",
    "g.add_legend()\n",
    "g.set_titles(col_template=\"{col_name}\")\n",
    "#g.set(xscale=\"log\")\n",
    "sns.set(font_scale = 1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(means, col=\"task_name\", hue=\"tag\", col_wrap=4, sharex=False, margin_titles=True, \n",
    "                  height=6, aspect=.75,\n",
    "                  col_order=[\"first_5_title\", \"first_5_wo_title\", \"first_10_title\", \"first_10_wo_title\",\n",
    "                            \"first_25_title\", \"first_100_title\", \"rand_25_title\", \"rand_100_title\",\n",
    "                            \"first_1_title\", \"title_only\"])\n",
    "g.map(sns.lineplot, \"trainsize\", \"rprec\", marker=\"o\", legend=\"full\", linewidth = 4)\n",
    "g.add_legend()\n",
    "g.set_titles(col_template=\"{col_name}\")\n",
    "#g.set(xscale=\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x = \"task_name\",       # x variable name\n",
    "            y = \"rprec\",       # y variable name\n",
    "            hue = \"tag\",  # group variable name\n",
    "            data = means,     # dataframe to plot\n",
    "            kind = \"bar\",\n",
    "            aspect = 2)\n",
    "ignore=plt.xticks(rotation=45)\n",
    "ignore=plt.title(\"R-Precision Compared\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(means, col=\"task_name\", hue=\"tag\", col_wrap=4, sharex=False, margin_titles=True, \n",
    "                  height=6, aspect=.75,\n",
    "                  col_order=[\"first_5_title\", \"first_5_wo_title\", \"first_10_title\", \"first_10_wo_title\",\n",
    "                            \"first_25_title\", \"first_100_title\", \"rand_25_title\", \"rand_100_title\",\n",
    "                            \"first_1_title\", \"title_only\"])\n",
    "g.map(sns.lineplot, \"trainsize\", \"ndcg\", marker=\"o\", legend=\"full\", linewidth = 4)\n",
    "g.add_legend()\n",
    "g.set_titles(col_template=\"{col_name}\")\n",
    "#g.set(xscale=\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[results[\"trainsize\"]==20]#.groupby([\"tag\"]).rprec.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.groupby([\"tag\"]).ndcg.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove rand100 which is the task which most significantly differentiates method performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[results.task_name!=\"rand_100_title\"].groupby([\"tag\"]).rprec.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[results.task_name!=\"rand_100_title\"].groupby([\"tag\"]).ndcg.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(results, row=\"task_name\", col=\"tag\", hue=\"tag\", margin_titles=True, height=3, aspect=4)\n",
    "g.map(sns.violinplot, \"tag\", \"rprec\", palette=\"muted\", inner=\"quart\", cut=0, orient='v', scale='count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create means as dataframe rather than groupby object by using the aggregator on the groupby object rather than the series of rprec column alone.  This keeps the column names and should simplify plotting.\n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x = \"task_name\",       # x variable name\n",
    "            y = \"rprec\",       # y variable name\n",
    "            hue = \"tag\",  # group variable name\n",
    "            data = means,     # dataframe to plot\n",
    "            kind = \"bar\",\n",
    "            aspect = 2)\n",
    "ignore=plt.xticks(rotation=45)\n",
    "ignore=plt.title(\"R-Precision Compared\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x = \"task_name\",       # x variable name\n",
    "            y = \"ndcg\",       # y variable name\n",
    "            hue = \"tag\",  # group variable name\n",
    "            data = means,     # dataframe to plot\n",
    "            kind = \"bar\",\n",
    "            aspect = 2)\n",
    "ignore=plt.xticks(rotation=45)\n",
    "ignore=plt.title(\"NDCG Compared\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x = \"task_name\",       # x variable name\n",
    "            y = \"rprec\",       # y variable name\n",
    "            hue = \"tag\",  # group variable name\n",
    "            data = means,     # dataframe to plot\n",
    "            #kind = \"scatter\",\n",
    "            #aspect = 2\n",
    "            )\n",
    "ignore=plt.xticks(rotation=45)\n",
    "ignore=plt.title(\"R-Precision Compared\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x = \"task_name\",       # x variable name\n",
    "            y = \"rprec\",       # y variable name\n",
    "            hue = \"tag\",  # group variable name\n",
    "            data = results,     # dataframe to plot\n",
    "            #kind = \"scatter\",\n",
    "            #aspect = 2\n",
    "            )\n",
    "ignore=plt.xticks(rotation=45)\n",
    "ignore=plt.title(\"R-Precision Compared\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Violin plot\n",
    "\n",
    "start with the routine used during the recsys18 analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_violin(df, title=\"Violin Plot\"):\n",
    "    #sns.set_style(\"white\") \n",
    "    sns.set(rc={'figure.figsize':(8,6)})\n",
    "    g = sns.violinplot(data=df, cut=0, orient='v', scale='width')\n",
    "    #g = sns.violinplot(x=df.iloc[,0], y=df.iloc[0,:], cut=0, scale='width')\n",
    "    g.set_title(title)\n",
    "    #g.set_xlabel(\"Subtask\")\n",
    "    g.set_ylabel(\"Score\")\n",
    "    g.set_xticklabels(g.get_xticklabels(), rotation=90)\n",
    "\n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't need the describe data because that will come from the violin plot.  Just need to use all the raw data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore side-by-side plots\n",
    "\n",
    "What to take all the tasks and see the teams side by side\n",
    "\n",
    "Create violin plots for each task. https://stackoverflow.com/a/47487445/8928529\n",
    "\n",
    "Basically loop through the tasks and plot on each subplot axis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fig, axes = plt.subplots(10, 2, figsize=(20, 32), sharey='row')\n",
    "axes_cols = (axes.flatten()[::2], axes.flatten()[1::2])\n",
    "\n",
    "i=0\n",
    "for task in vl6desc.task_name.drop_duplicates():\n",
    "    \n",
    "    ax=axes_cols[0][i]\n",
    "    sns.violinplot(data=vl6desc[vl6desc[\"task_name\"]==task], cut=0, orient='v', scale='width', ax=ax)\n",
    "    ax.set_title('task = {}'.format(task), y=0.95)\n",
    "    ax=axes_cols[1][i]\n",
    "    sns.violinplot(data=hwdesc[hwdesc[\"task_name\"]==task], cut=0, orient='v', scale='width', ax=ax)\n",
    "    ax.set_title('task = {}'.format(task), y=0.95)\n",
    "    \n",
    "    \n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore split plot\n",
    "\n",
    "This lets me see the data paired directly and allows easier visual comparison of differences.\n",
    "\n",
    "The test rand_100_title task shows clear differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the rprec and ndcg data plotted side by side it is clear that the rand_100_title was much more effectively solved by vl6.\n",
    "\n",
    "The second most effective was first_1_title where vl6 had higher mean.\n",
    "\n",
    "However the the title_only solution of hw is clearly better.\n",
    "\n",
    "All the rest of the tasks had nearly identical means and distributions with the hw solution having slightly higher ndcg in those tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's much easier to get fast insight from seaborn using facet grids since they are built right from the data.\n",
    "\n",
    "https://seaborn.pydata.org/tutorial/axis_grids.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(results, row=\"task_name\", col=\"tag\", hue=\"tag\", margin_titles=True, height=4, aspect=2)\n",
    "g.map(sns.scatterplot, \"pid\", \"rprec\", linewidth = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"rpid\"]= data.pid % 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[\"rpid\"] = results.pid % 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(results, row=\"task_name\", col=\"tag\", hue=\"tag\", margin_titles=True, height=4, aspect=2)\n",
    "g.map(sns.scatterplot, \"rpid\", \"rprec\", linewidth = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Revisit violin distribution plots now with all data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order = [\"first_5_title\", \"first_5_wo_title\", \"first_10_title\", \"first_10_wo_title\", \"first_25_title\", \"first_100_title\", \"rand_25_title\", \"rand_100_title\", \"first_1_title\", \"title_only\"]\n",
    "\n",
    "g = sns.FacetGrid(results, col=\"task_name\",  col_order=order, margin_titles=True, height=5, col_wrap=2, aspect=1)\n",
    "g.map(sns.violinplot, \"tag\", \"rprec\", palette=\"muted\", inner=\"quart\", cut=0, orient='v', scale='count')\n",
    "g.add_legend()\n",
    "plt.xticks(rotation=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order = [\"first_5_title\", \"first_5_wo_title\", \"first_10_title\", \"first_10_wo_title\", \"first_25_title\", \"first_100_title\", \"rand_25_title\", \"rand_100_title\", \"first_1_title\", \"title_only\"]\n",
    "\n",
    "g = sns.FacetGrid(data, col=\"task_name\",  hue = \"tag\", col_order=order, margin_titles=True, height=5, col_wrap=2, aspect=1)\n",
    "g.map(sns.kdeplot, data=data, x=\"rprec\", hue=\"tag\", palette=\"muted\", cut=0, fill=False)\n",
    "g.add_legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Inspect the highest scoring recommendations\n",
    "\n",
    "Understand which tasks and methods perform the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[results.rprec > 0.8][[\"rprec\", \"ndcg\", \"tag\",\"task_name\", \"name\", \"num_tracks\"]]"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
