{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User-to-user Collaborative Filtering\n",
    "\n",
    "Create a basic user-to-user K-nearest neighbors (u2uknn) collaboritive filtering solution for recommended tracks.\n",
    "This is built from the Verstrepen2017 matrix notation of a score matrix factored by user-to-user similarity and the ratings matrix.\n",
    "\n",
    "    Score = Similarity x Ratings \n",
    "    \n",
    "The similarity and ratings matrix are built from an append of the challenge set to the training set.\n",
    "This is required by KNN because the similarity computation is of each individual challenge playlist against all the training set.\n",
    "In this formulation we technically also include the other training set similarities in the final score.\n",
    "This simplifies the computation and dimension managment and avoids having to loop on individual challenge playlist and training set pairings.\n",
    "We assume that the impact on final scores is minimal.\n",
    "\n",
    "The cold start task is address with a simple global popularity ranking of tracks.\n",
    "We compute the global popularity of tracks and recommend the first 500 in ranked order.\n",
    "This set is also included as backfill to each playlist recommendation accross all subtasks to ensure the minimum 500 tracks are available to each challenge solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import re\n",
    "import collections\n",
    "import os\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date, datetime\n",
    "import utils\n",
    "from utils import tic, toc\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set parameters for Run\n",
    "\n",
    "These parameters control the selection of dataset, challenge set, and solution output.\n",
    "\n",
    "Must select dataset and challege_name.  The rest are derived parameters and can be left alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the directory with different collections of training and test sets\n",
    "datadir=\"data/\"  # path to base data set\n",
    "resultsdir=\"\"\n",
    "\n",
    "# select the data for training  \n",
    "#dataset=\"mpd/data\"     # the original full mpd training set, requires quick=True and max_files set\n",
    "#dataset=\"mpd-1st-21k\"  # first 21 files of mpd, equiv to quick=True and max_files 20\n",
    "#dataset=\"mpd-2nd-21k\"  # second 21 files of mpd, avoids using data that built mympd challenge set\n",
    "dataset=\"mympd-full-20k\"\n",
    "\n",
    "# select data for testing, this is the challenge_set.json file for specific challenge data\n",
    "#challenge_name=\"mpd\"  # original mpd challenge set, use with aicrowd\n",
    "#challenge_name=\"mympd\"  # my custom challenge set for task analysis\n",
    "challenge_name=\"mympd-full\"  # my custom challenge sampled from full training set for task analysis\n",
    "\n",
    "# optionally tag the run\n",
    "# this can be used to group the results file with other run artifacts, eg. job info\n",
    "tagname=\"\"\n",
    "\n",
    "\n",
    "# provide method selector to do u2u or i2i collaborative filtering\n",
    "method=\"nn-ae\"\n",
    "\n",
    "# train against tracks with minimum support, all by default\n",
    "minsupport = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set the model parameters and then train\n",
    "\n",
    "We can control the batch size with the data loaders.  An can set the other parameters based on exising models of interest. (e.g. hello world)\n",
    "\n",
    "    [DAE]\n",
    "    epochs = 20\n",
    "    batch = 250\n",
    "    lr = 0.005\n",
    "    reg_lambda = 0.0\n",
    "    hidden = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Parameters from hello world! DAE\n",
    "# \n",
    "epochs = 20\n",
    "batch_size = 250\n",
    "learning_rate = 0.005"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "source": [
    "The papermill parameter cell needs to set variables exclusively before they can be used.  The CLI parameters are injected in a cell after the parameters cell.  The parameters can't be used until after the injected cell, otherwise they just get the static default values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# derived parameters, no need to change.\n",
    "\n",
    "if (len(tagname)>0): tagname=\"_\"+tagname\n",
    "\n",
    "#trainset=\"data/mpd/data\"  # relative path to training set in slice json file format\n",
    "trainset=datadir+dataset+\"/data\"  # relative path to training set in slice json file format\n",
    "\n",
    "#testset=\"data/challenge_set.json\" # relative path to test set in challenge_set.json format\n",
    "#testset=datadir+challenge_name+\"-challenge-set/challenge_set.json\" # relative path to test set in challenge_set.json format\n",
    "testset=datadir+challenge_name+\"/challenge_set.json\" # relative path to test set in challenge_set.json format\n",
    "\n",
    "datestr=datetime.now().strftime('%Y-%m-%d-%H:%M:%S')\n",
    "\n",
    "challenge_header = \"team_info,jprorama,jprorama@gmail.com\\n\"\n",
    "challenge_solution = resultsdir+\"method-\"+method+\"_\"+challenge_name+\"_\"+dataset+\"_\"+datestr+tagname+\".csv\"\n",
    "\n",
    "# checkpoint file for model\n",
    "modelchk = resultsdir+\"/cache/\"+\"method-\"+method+\"_\"+challenge_name+\"_\"+dataset+\".pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate more verbose output for some functions\n",
    "debug = True\n",
    "\n",
    "# parameters for mpd load, superceeded by mpd-1st-21k\n",
    "quick = False\n",
    "max_files_for_quick_processing = 20\n",
    "\n",
    "# random state\n",
    "seed = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the mpd training data\n",
    "\n",
    "Create data frame for playlists and tracks to make it simple to work with the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "playlists, tracks = utils.process_mpd(trainset, quick, maxfiles=max_files_for_quick_processing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set a new index for playlists so each row has unique id using pid. After reading the slice files the index values repeat for each slice.\n",
    "\n",
    "Preference is to not use the pid since that drops this data column.\n",
    "Instead create a new column of integers for each row and then set that as the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "playlists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset_size=playlists.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl = playlists.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl = playlists[[\"pid\",\"tracks\"]].explode(\"tracks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl[\"track_uri\"] = [d.get(\"track_uri\") for d in pl.tracks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl[\"artist_name\"] = [d.get(\"artist_name\") for d in pl.tracks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expanded one-row-per-track representation shows we have 1.4million songs (rows). The row index has 21k entries which matches the 21k playlists in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks = pl[[\"track_uri\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks.memory_usage(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks.memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From example in [pandas sparse data types page](https://pandas.pydata.org/pandas-docs/stable/user_guide/sparse.html) use memory_usage().sum().  Not clear why we divide by 1000.  Would think that makes it kilobytes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'dense : {:0.2f} kbytes'.format(tracks.memory_usage().sum() / 1e3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot encode playlists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attempting to use get_dummies() works in the dense space an tries to build a dataframe of 100k by 1.4Million songs.  Not sure why so many rows but it's still to big for ram at 300+G\n",
    "\n",
    "trackhots = pd.get_dummies(tracks, dtype=bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sklearn has a onehot encoder that is a preprocessor to many of its routines.  See if we can fit the tracks to this representaiton.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pl[[\"pid\", \"track_uri\"]].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trackhots = OneHotEncoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trackhots.fit(pl[[\"pid\", \"track_uri\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trackhots.categories_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trackhots.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the original data into a matrix representation.\n",
    "\n",
    "Here again is the 1.4x290k represenation.  The 1.4k is the songs, so rows in the original matrix but not clear where the 290k comes from.  Would expect 21k for the playlists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "th = trackhots.transform(pl[[\"pid\",\"track_uri\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "th"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pl[\"pid\"].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "playlists[\"num_tracks\"].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm, there are some problems in the transformation.  The 1.4mil comes from the total number of tracks in training.  The total unique is much smaller."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pl[\"track_uri\"].drop_duplicates().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'd expect an transformed data set to be 21k by 269k.\n",
    "\n",
    "Ah, the onehot encoder wants a feature set of each record with its distinct features.\n",
    "https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing-categorical-features\n",
    "\n",
    "in this case it's rows of track_uri.\n",
    "so each row with mapp to the idx value and will just have tracks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "playlists.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pl[[\"track_uri\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try converting each tracks string to a data type \n",
    "\n",
    "https://www.geeksforgeeks.org/python-convert-string-dictionary-to-dictionary/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "playlists[[\"tracks\"]].tracks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a list of lists. This is pretty easy to construct with a list comprehension to wrap the lists into a list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pltracks = [d for d in playlists[[\"tracks\"]].tracks.apply((lambda s: [d[\"track_uri\"] for d in s]))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "len(pltracks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "type(pltracks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what we are really trying to do is train the encoding and then transform each row.\n",
    "\n",
    "this is more like having a vocabulary and different sentances.\n",
    "I need to map each sentance to it's onehot encoding of the vocabulary.\n",
    "\n",
    "this example shows moving from an integerencoding to a one hot encoding\n",
    "https://machinelearningmastery.com/how-to-one-hot-encode-sequence-data-in-python/\n",
    "\n",
    "reading the docs leads to multi label binarizer which appears to be closer to what i want.\n",
    "https://scikit-learn.org/stable/modules/preprocessing_targets.html#multilabelbinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer(sparse_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pltracks = mlb.fit_transform(pltracks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally have a list of 21k playlists encoded with the 269k unique tracks.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pltracks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Cosine similarity\n",
    "\n",
    "https://stackoverflow.com/a/27046041/8928529"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sim = cosine_similarity(pltracks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sim.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to do a matrix multiply for user-user similarity: score = sim * ratings\n",
    "\n",
    "https://stackoverflow.com/a/16754459/8928529"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cast the similarity martrix into a compressed sparse row format so matrix multiplication doesn't explode the ram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sim = sparse.csr_matrix(sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "score = sparse.csr_matrix.dot(sim, pltracks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is a score matrix in the original dimentions that is 17% sparse.  With 973mil out of 5billion possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "score.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Append Challenge Set to Training Set\n",
    "\n",
    "Read the data from the challenge set file and append the tracks to training tracks in prep for similarity comparison. Omit first 1000 tracks since this is the title only subtask. Their similarity is implicitly zero on all tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data using Python JSON module\n",
    "with open(testset,'r') as f:\n",
    "    data = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten data\n",
    "challenge_playlists = pd.json_normalize(data, record_path=['playlists'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[challenge_playlists[\"tracks\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chtracks = [d for d in challenge_playlists[[\"tracks\"]].tracks.apply((lambda s: [d[\"track_uri\"] for d in s]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chtracks[1002]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pltracks = [d for d in playlists[[\"tracks\"]].tracks.apply((lambda s: [d[\"track_uri\"] for d in s]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pltracks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chtracks[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alltracks = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alltracks = pltracks + chtracks[1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(alltracks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory size of matrices\n",
    "\n",
    "https://pretagteam.com/question/determining-the-byte-size-of-a-scipysparse-matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_bytes(a):\n",
    "    return a.data.nbytes + a.indptr.nbytes + a.indices.nbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parts_bytes(a):\n",
    "    return [a.data.nbytes, a.indptr.nbytes, a.indices.nbytes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parts_types(a):\n",
    "    return [a.data.dtype, a.indptr.dtype, a.indices.dtype]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def in_megs(n):\n",
    "    \n",
    "    MB=n/1024/1024\n",
    "    return \"{:6.2f} MB\".format(MB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def size_report(a, name=\"matrix\"):\n",
    "    print(\"shape of {}: {}\".format(name, a.shape))\n",
    "    print(\"nnz of {}: {}\".format(name, a.nnz))\n",
    "    print(\"sparsity of {}: {:3.4f} %\".format(name, 100*(1-a.nnz/(a.shape[0]*a.shape[1]))))\n",
    "    print(\"size of {}: {}\".format(name, in_megs(sparse_bytes(a))))\n",
    "    print(\"size of {} parts: data: {}, indptr: {}, indices: {}\".format(name, *map(in_megs, parts_bytes(a))))\n",
    "    print(\"type of {} parts: data: {}, indptr: {}, indices: {}\".format(name, *parts_types(a)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Sparse Binary User-Item Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allmpd = mlb.fit_transform(alltracks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allmpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_report(allmpd, \"all_ratings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter tracks without minimum support\n",
    "\n",
    "By default we use all tracks to build our models.  This doesn't necessarily make sense for tracks that appear in only one playlist since these are not good candidates for recommendation.  How do you recommend something that only one person ever liked. By definition these tracks would not show up in another playlist.\n",
    "\n",
    "These tracks are also expensive to maintain in the model because they account for almost half of the tracks in a sample.  Including them, therefore, almost doubles the lenth of the vectors we must maintain in in our models.  By elliminiting these tracks we significantly reduce the memory footprint of the weight matrices in our models.\n",
    "\n",
    "It's easiest to discover these tracks by summing our initial non-filtered collection of tracks and using that as input to a second pass at a model that has a restricted set of classes given to the multi-label binarizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (minsupport > 0):\n",
    "    \n",
    "    # create boolean filter for \n",
    "    minsupport_filter = allmpd.sum(axis=0)>minsupport\n",
    "    minsupport_filter = np.squeeze(np.asarray(minsupport_filter))\n",
    "\n",
    "    print(f\"original track count: {len(mlb.classes_)}\")\n",
    "    print(f\"filtered track count: {len(mlb.classes_[minsupport_filter])}\")\n",
    "\n",
    "    mlb_filter = MultiLabelBinarizer(classes=mlb.classes_[minsupport_filter], sparse_output=True)\n",
    "\n",
    "    allmpd = mlb_filter.fit_transform(alltracks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allmpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_report(allmpd, \"all_ratings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_report(allmpd[0:trainset_size], \"train_ratings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_report(allmpd[trainset_size:], \"test_ratings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a train-test split\n",
    "\n",
    "The AENN needs a test set to asses performance during training.\n",
    "\n",
    "We follow the lead of hello-world! and use a 3k test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allmpd_train, allmpd_test = train_test_split(allmpd[:trainset_size], test_size=3000, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allmpd_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allmpd_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dataset and loader for torch\n",
    "\n",
    "The dataset and loader are based on this [example in the tutorial](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html).  The examples describe loading data from files within the dataset \\__getitem__ but we already have all that parsing complete.  We can simply load the result parsed data into the class and convert it to a sparse tensor.  This makes it easy for the dataset and dataloader to iterate through the object for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class allmpdDataset(Dataset):\n",
    "    def __init__(self, mpd):\n",
    "        '''\n",
    "        convert the input matrix from scipy csr to torch sparse via COO intermediary format\n",
    "        https://discuss.pytorch.org/t/creating-a-sparse-tensor-from-csr-matrix/13658/5\n",
    "        \n",
    "        Note the explicit dimension setting arg via the coo.shape. This ensures\n",
    "        the matrix doesn't lose dimensions due to the max column values being different.\n",
    "        This occurs if the max column id happens to be less that the global max, ie. \n",
    "        a missing max item id in one of the conversions.\n",
    "        '''\n",
    "        coo = mpd.tocoo()\n",
    "        values = coo.data\n",
    "        indices = np.vstack((coo.row, coo.col))\n",
    "\n",
    "        i = torch.LongTensor(indices)\n",
    "        v = torch.FloatTensor(values)\n",
    "        shape = coo.shape\n",
    "\n",
    "        self.mpd = torch.sparse.FloatTensor(i, v, torch.Size(shape))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.mpd.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):        \n",
    "        return self.mpd[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate the class with the parsed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpd_train = allmpdDataset(allmpd_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpd_test = allmpdDataset(allmpd_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the dataloader to iterrate though the mpd dataset. Start with a small batch to observe operation. Start with non-random iteration for repeatability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_train = DataLoader(mpd_train, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_test = DataLoader(mpd_test, batch_size=64, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the first batch of data as confirmation of working iterable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(dl_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(dl_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Simple AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the AutoEncoder class\n",
    "\n",
    "Build the network using the layers proved out above.  The structure is driven by a desire to build a basic encoder-decoder for playlists.  The network is designed to use just the user-item matrix as represented by playlist-song data.  No additional data is used in order to facilitate direct comparison to mf-als and the knn methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AE(nn.Module):\n",
    "    def __init__(self, in_features, embedding=256):\n",
    "        super(AE, self).__init__()\n",
    "        \n",
    "        self.aestack = nn.Sequential(\n",
    "            nn.Linear(in_features=in_features, out_features=embedding),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(in_features=embedding, out_features=in_features),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.aestack(x)\n",
    "        \n",
    "                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model and data need to be on the same device (gpu or cpu) in order to run through the network via the forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the model model\n",
    "\n",
    "We can access all the parameters of the model at each layer.\n",
    "https://discuss.pytorch.org/t/access-all-weights-of-a-model/77672/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f\"name: {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size the model weights and biases take up on the gpu grows as the size of the input grows.\n",
    "The input is sparse but the wieght matrices become dense.  Additionally the output of the batch is dense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_size_report(model):\n",
    "    w1bytes = model.aestack[0].weight.size()[0] * model.aestack[0].weight.size()[1] * 4\n",
    "    b1bytes = model.aestack[0].bias.size()[0] * 4\n",
    "    w2bytes = model.aestack[2].weight.size()[0] * model.aestack[2].weight.size()[1] * 4\n",
    "    b2bytes = model.aestack[2].bias.size()[0] * 4\n",
    "    outbytes = batch_size * model.aestack[2].weight.size()[0] * 4\n",
    "\n",
    "    print(f\"w1bytes: {w1bytes}\")\n",
    "    print(f\"b1bytes: {b1bytes}\")\n",
    "    print(f\"w2bytes: {w2bytes}\")\n",
    "    print(f\"b2bytes: {b2bytes}\")\n",
    "    print(f\"modelbytes: {w1bytes+b1bytes+w2bytes+b2bytes}\")\n",
    "    print(f\"outbytes: {outbytes}\")\n",
    "    print(f\"batchsize: {batch_size}\")\n",
    "    print(f\"totalgpubytes: {w1bytes+b1bytes+w2bytes+b2bytes+outbytes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model_size_report(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Training and Test Loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = False\n",
    "if (device == \"cuda\"):\n",
    "    memuse = True\n",
    "else:\n",
    "    memuse = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer, epoch=1):\n",
    "    global memuse\n",
    "    global modelchk\n",
    "\n",
    "    start_event = torch.cuda.Event(enable_timing=True)\n",
    "    end_event = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, X in enumerate(dataloader):\n",
    "                \n",
    "        if (batch > 3): memuse = False\n",
    "\n",
    "        if (memuse):\n",
    "            start_event.record()\n",
    "\n",
    "        # Compute prediction and loss\n",
    "        if (memuse): print(f\"gpu mem pre-batch: {torch.cuda.memory_allocated()}\")\n",
    "        X = X.to(device)\n",
    "        if (memuse): print(f\"gpu mem post-batch: {torch.cuda.memory_allocated()}\")\n",
    "        pred = model(X)\n",
    "        if (memuse): print(f\"gpu mem post-pred: {torch.cuda.memory_allocated()}\")\n",
    "\n",
    "        X = X.to_dense()\n",
    "        if (memuse): print(f\"gpu mem post-X dense: {torch.cuda.memory_allocated()}\")\n",
    "        pred = pred.to(device)\n",
    "        if (memuse): print(f\"gpu mem post-pred to device: {torch.cuda.memory_allocated()}\")\n",
    "\n",
    "        if (debug):\n",
    "            print(f\"X type: {type(X)}\")\n",
    "            print(f\"pred type: {type(pred)}\")\n",
    "            print(f\"X is sparse: {X.is_sparse}\")\n",
    "            print(f\"pred is sparse: {pred.is_sparse}\")\n",
    "            # size of tensor\n",
    "            # https://discuss.pytorch.org/t/how-to-know-the-memory-allocated-for-a-tensor-on-gpu/28537/2\n",
    "            print(f\"X size: {X.element_size() * X.nelement()}\")\n",
    "            print(f\"pred size: {pred.element_size() * pred.nelement()}\")\n",
    "\n",
    "       \n",
    "        # The autoencoder trains against the input samples\n",
    "        if (memuse): print(f\"gpu mem pre-loss: {torch.cuda.memory_allocated()}\")\n",
    "        loss = loss_fn(pred, X)\n",
    "        if (memuse): print(f\"gpu mem post-loss: {torch.cuda.memory_allocated()}\")\n",
    "\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        if (memuse): print(f\"gpu mem pre-backward: {torch.cuda.memory_allocated()}\")\n",
    "        loss.backward()\n",
    "        if (memuse): print(f\"gpu mem post-backward: {torch.cuda.memory_allocated()}\")\n",
    "        optimizer.step()\n",
    "        if (memuse): print(f\"gpu mem post-step: {torch.cuda.memory_allocated()}\")\n",
    "\n",
    "        if (memuse):\n",
    "            end_event.record()\n",
    "            torch.cuda.synchronize()  # Wait for the events to be recorded!\n",
    "            elapsed_time_ms = start_event.elapsed_time(end_event)\n",
    "            print(f\"elapsed time ms: {elapsed_time_ms}\")\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "            \n",
    "    # checkpoint at end of epoch\n",
    "    torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss,\n",
    "            }, modelchk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X in dataloader:\n",
    "            if (debug):\n",
    "                print(f\"X test size: {X.shape}\")\n",
    "            X = X.to(device)\n",
    "            if (debug):\n",
    "                print(f\"X test size: {X.shape}\")\n",
    "            pred = model(X)\n",
    "            pred = pred.to(device)\n",
    "            X = X.to_dense()\n",
    "\n",
    "            test_loss += loss_fn(pred, X).item()\n",
    "            correct += (pred.argmax(0) == X).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the model from a fresh start with multiple epochs of training\n",
    "\n",
    "Now we can observe the impact of the learning rate, loss function and optimizer selected above in a full training run.\n",
    "\n",
    "First we reset the model to start the training process from the initial point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AE(allmpd_train.shape[1]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_train = DataLoader(mpd_train, batch_size=batch_size, shuffle=False)\n",
    "dl_test = DataLoader(mpd_test, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_size_report(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.memory_allocated()/1024/1024/1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.memory_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_epoch = 0\n",
    "if (os.path.exists(modelchk)):\n",
    "    checkpoint = torch.load(modelchk)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    cur_epoch = checkpoint['epoch']\n",
    "    loss = checkpoint['loss']\n",
    "    \n",
    "    print(f\"Epoch {cur_epoch}\\n-------------------------------\")\n",
    "    print(f\"loss: {loss:>7f}\")\n",
    "    \n",
    "    if (cur_epoch < epochs):\n",
    "        # continue training\n",
    "        model.train()\n",
    "    else:\n",
    "        # ready for inference\n",
    "        model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for t in range(cur_epoch, epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    if (device == \"cuda\"): memuse = True\n",
    "    train_loop(dl_train, model, loss_fn, optimizer, epoch=t+1)\n",
    "    test_loop(dl_test, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assess the quality of the learning\n",
    "\n",
    "Sum the weights and biases to confirm that they changed since before training.\n",
    "\n",
    "Also compare the number of unique weight values to the total possible values for the trained model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1sum_post = model.aestack[0].weight.sum()\n",
    "b1sum_post = model.aestack[0].bias.sum()\n",
    "w2sum_post = model.aestack[2].weight.sum()\n",
    "b2sum_post = model.aestack[2].bias.sum()\n",
    "\n",
    "print(f\"w1 sum: {w1sum_post}\")\n",
    "print(f\"b1 sum: {b1sum_post}\")\n",
    "print(f\"w2 sum: {w2sum_post}\")\n",
    "print(f\"b2 sum: {b2sum_post}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.aestack[0].weight.size()[0] * model.aestack[0].weight.size()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model.aestack[0].weight.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model.aestack[2].weight.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Recommendations from model\n",
    "\n",
    "### Create a dataset and data loader for the challenge tracks.\n",
    "\n",
    "The create a function to loop through the challenge dataset and gather the recomendations.\n",
    "\n",
    "The recommendations can then be used directly to produce recommendations via torch.argsort() which produces the index of entries in sorted order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allmpd[trainset_size:][0].indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpd_chall = allmpdDataset(allmpd[trainset_size:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_chall = DataLoader(mpd_chall, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visually inspect samples at 0 and 1000 to ensure that challenge set mapping is accurate\n",
    "\n",
    "We want to make sure that the wrapping of the allmpd challenge track remains consistent in all its different representations: scipy sparse, sparse tensor, and data loader wrapper.\n",
    "\n",
    "Inspect index 0 and index 1000 to confirm they contain the same track id values as their index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allmpd[trainset_size:][0].indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpd_chall[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(dl_chall))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allmpd[trainset_size:][1000].indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpd_chall[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the iterations and offset to get to \n",
    "# challenge playlist 1000 with the dataloader\n",
    "\n",
    "batch = 1000 // dl_chall.batch_size\n",
    "offset = 1000 - (dl_chall.batch_size * batch)\n",
    "\n",
    "for i, X in enumerate(dl_chall):\n",
    "    if (i==batch):\n",
    "        print(X[offset])\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the Challenge recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rec_loop(dataloader, model):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    \n",
    "    # build a list of recs from the batch loop\n",
    "    # https://pytorch.org/docs/stable/generated/torch.cat.html#torch-cat\n",
    "    recs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, X in enumerate(dataloader):\n",
    "            if (debug):\n",
    "                print(f\"X test size: {X.shape}\")\n",
    "            X = X.to(device)\n",
    "            if (debug):\n",
    "                print(f\"X test size: {X.shape}\")\n",
    "            recs.append(model(X).to(\"cpu\"))\n",
    "            \n",
    "    return torch.cat(recs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recs = rec_loop(dl_chall, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the challenenge recommendations\n",
    "\n",
    "The dimensions are correct but all the track recommendations are identical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recs.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recs[0].argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recs[1].argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recs[1000].argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recs[1001].argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.aestack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the sort() an argsort() return their results in ascending order by score. That means it's the lowest to highest score, or worst to best recommendation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recs[0].sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recs[0].argsort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the descending arg to reverse the sort order and return the preferred result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recs[0].sort(descending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recs[0].argsort(descending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Top-N Tracks\n",
    "\n",
    "Use the global top-N tracks to backfill missing data for playlist recommendation.\n",
    "The top-n global also forms a baseline recommender to assess overall performance.\n",
    "\n",
    "We focus on the global track stats for the training dataset only. It makes sense because this is the known data but also because the final submission requires removal of challenge seed tracks so any track information learned from the challenge set can't be reused in the same list. Worth questioning some since it can't be reused in the same seed but could be in other playlists...\n",
    "\n",
    "However, do we need the full corpus so that we can match up the vocabulary terms to the tracks?  No because we can just use the Vecotrizors vocab.\n",
    "\n",
    "Approach is to treat the training playlists as corpus of text documents.  Count the occurance of terms with the count vectorizer.  Then sum the term counts into a single array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consruct corpus from playlists joined a list of strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[ \" \".join(doc) for doc in alltracks[0:2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a custom tokenizer to avoid splitting on the punctuation characters in the \"spotify:track:xxx' pattern. https://stackoverflow.com/a/37884104/8928529"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(tokenizer=lambda x: x.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trackcounts = vectorizer.fit_transform([\" \".join(doc) for doc in alltracks[0:20000]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trackcounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.sparse.from_spmatrix(trackcounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trackfreq = trackcounts.T.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(trackfreq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trackfreq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(trackfreq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trackfreq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = [(\"trackidx\", int), (\"count\", int)]\n",
    "new = np.empty(len(trackfreq), dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([*range(len(trackfreq))]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new['trackidx'] = np.array([*range(len(trackfreq))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(trackfreq).reshape(len(trackfreq)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new['count'] = np.array(trackfreq).reshape(len(trackfreq))\n",
    "#print new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuples = zip(new[\"trackidx\"], new[\"count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topn = sorted(tuples, key=lambda x: (x[1]), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Invert the vocabulary dictionary so we can map it to the tracks. https://stackoverflow.com/a/483833/8928529"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_map = {v: k for k, v in vectorizer.vocabulary_.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a list of the topn track identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toptracks=[inv_map[i] for i, cnt in iter(topn)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toptracks = toptracks[0:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top tracks can now be appended to each list an ensure we have the minimum recommenadable track set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore filtering challenge tracks from recommendation list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the challenge tracks from the recommended set.\n",
    "Use a simple loop for now to keep the code simple.\n",
    "Also allows us to inspect where the original songs are in the recommendation set.\n",
    "For the current playlist, app positions for the first 5 songs are above the 500 song rec limit.\n",
    "This suggests we will see a fairly poor rprec and ndcg performance for pure user-user knn.\n",
    "Makes sense, since this is really just a most popular songs amoung similar users strategey.\n",
    "A user focused popularity ranking rather than a global popularity ranking.\n",
    "Suggests the need for the boosting strategies we see in the actually top performers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "len(rectracks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean up the entire recommendation set. This is lists 21000-30000 in the currrent method. No index math is needed if we shift to putting the challenge tracks at the start.\n",
    "\n",
    "Trim the recommenation set out of the score results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even with the del earlier the RES memory remains  at 28g which helps explain why the next step kills the kernel\n",
    "\n",
    "    98895 jpr       20   0   29.9g  28.7g  29564 S   0.0 15.3   4:06.54 python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even with explicit garbage collection this doesn't free up the ram.\n",
    "\n",
    "https://stackoverflow.com/questions/1316767/how-can-i-explicitly-free-memory-in-python\n",
    "\n",
    "Advice is to use a subprocess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe it means I need to save out the results and reload them either in a new notebook or after the kernel barfs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pickle.dump(score, open( \"save_score.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "score=pickle.load(open( \"save_score.p\", \"rb\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even pickle kills the kernel. \n",
    "Maybe best to just add some ram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the challenge tracks from the recommendation list.  The recommendation list is augmented with the topn (n=1000) most popular tracks to backfill recommendations that don't have enough tracks to fill the 500 count requirement.\n",
    "\n",
    "The algorithm slows noticably as the number of challenge tracks increases and the recommendation list has to be search repeatedly.\n",
    "\n",
    "The 9000 scored playlists start for playlist 1000.\n",
    "The challenge playlist starts with the first playlist.\n",
    "Need to offset the challenge playlist index to match the score structure and recommended tracks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "import time\n",
    "\n",
    "reclist = list()\n",
    "indexdist = list() #pd.DataFrame(columns=[\"index\"])\n",
    "misses = 0\n",
    "tooshort = 0\n",
    "trace = False\n",
    "timeit=False\n",
    "\n",
    "start_time = time.time()\n",
    "for idx in range(9000):\n",
    "    # get the candidate track class id sorted by recommendation score\n",
    "    if (timeit): tic()\n",
    "    cantracks = recs[idx].argsort(descending=True).tolist()[0:750]\n",
    "    if (timeit): toc(\"cantracks\")\n",
    "    \n",
    "    # convert class id to spotify track name\n",
    "    if (timeit): tic()\n",
    "    rectracks=[mlb.classes_[i] for i in cantracks]\n",
    "    if (timeit): toc(\"rectracks\")\n",
    "    \n",
    "    if (timeit): tic()\n",
    "    # ensure mininum length recommendation meets 500 tracks requirement\n",
    "    rectracks=rectracks + toptracks\n",
    "    # truncate rectracks to top1000 to limit search effort for challenge tracks\n",
    "    rectracks=rectracks[:1000]\n",
    "    if (timeit): toc(\"rectracks_1k\")\n",
    "    if (trace): print(\"idx {}:\".format(idx))\n",
    "    # remove challenge tracks from recommendation list\n",
    "    # note: the challenge tracks start at index 1000 to allign tasks with recommendations\n",
    "    \n",
    "    # convert to dataframe to benefit from vector operations\n",
    "    # impoves about 2x over speed of manual loops.\n",
    "    if (timeit): tic()\n",
    "    recdf = pd.DataFrame(rectracks, columns=[\"track\"])\n",
    "    if (timeit): toc(\"recdf\")\n",
    "    if (timeit): tic()\n",
    "    filterlist = recdf[recdf.track.isin(chtracks[idx+1000])].index\n",
    "    if (timeit): toc(\"filterlist\")\n",
    "    # need to record index collection for distribution analysis but this isn't the correct approach\n",
    "    #indexdist.append(filterlist)\n",
    "    if (timeit): tic()\n",
    "    recdf.drop(filterlist, inplace=True )\n",
    "    if (timeit): toc(\"recdf.drop\")\n",
    "    if (timeit): tic()\n",
    "    rectracks = recdf[\"track\"].tolist()\n",
    "    if (timeit): toc(\"recdf.tolist\")\n",
    "\n",
    "    #for challenge_track in chtracks[idx+1000]:\n",
    "    #    if (trace): print(\"look for track: {}\".format(challenge_track))\n",
    "    #    while challenge_track in rectracks:\n",
    "    #        try:\n",
    "    #            indexdist.append(rectracks.index(challenge_track))\n",
    "    #            if (trace): print(\"remove track pos: {}\".format(rectracks.index(challenge_track)))\n",
    "    #            rectracks.remove(challenge_track)\n",
    "    #        except (ValueError, AttributeError):\n",
    "    #            if (trace): print(\"didn't find in rectracks: {}\".format(challenge_track))\n",
    "    #            misses += 1\n",
    "\n",
    "    \n",
    "    #if reclist < 500:\n",
    "    #    tooshort += 1\n",
    "    \n",
    "    # truncate recommendation list to the 500 length required\n",
    "    if (timeit): tic()\n",
    "    reclist.append(rectracks[0:500])\n",
    "    if (timeit): toc(\"reclist.append\")\n",
    "    \n",
    "    # progress bar\n",
    "    if (idx % 1000) == 0:\n",
    "        print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "        print(\"challenge tracks progress: {}\".format(idx))\n",
    "        start_time = time.time()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The index distribution is a simple peek into the performance of the KNN algorithm.  It shows the average index value of the seeded challenge tracks found in the recommendations.  Given that this number is dominated by values above the general playlist lengths and even above 500 it's clear that the similarity measure alone is not an effective way of organizing the recommendation results.\n",
    "\n",
    "Correction: the challenge set index was misalligned with the recommendation set index.  After updating the index to start after the title only task (+1000) the collection of matches shifted to where 70% of tracks were found in the first 500 recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "indexdist = pd.DataFrame(indexdist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of removals shows that the vast majority are well above the 500 reclist limit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "indexdist.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(reclist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "misses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(reclist[8995])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(reclist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# add the top popular to flesh out recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toptracks[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coldstart = pd.DataFrame(columns=[*range(500)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coldstart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coldstart = coldstart.append([toptracks[0:500]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coldstart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# create data frame of 1000 copies of top popular to substitute for title only cold start recommendation\n",
    "\n",
    "coldstart = pd.DataFrame(columns=[*range(500)])\n",
    "\n",
    "for i in range(1000):\n",
    "    coldstart = coldstart.append([toptracks[0:500]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coldstart.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solution = coldstart.append(submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solution.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reset the index so all rows have a distinct index value.  This is necessary so the pid insert below correctly adds each pid to the title only task.  Otherwise the repeated rows of the task are seen as a single distinct row and all get the same pid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solution = solution.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add playlist id into the submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[*range(10,10,1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generate playlist ID value range to create correct submission format.  We don't want to use a naive range of ids like the following\n",
    "\n",
    "    pid = pd.DataFrame([*range(1000000,1010000)]) \n",
    "\n",
    "We want to use the original pids from the challenge set.\n",
    "The pid is not arbitrary and should match the order of the pid in the challenge_playlists.  \n",
    "The rows of the solution are in the order of the original challenge set to should apply directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pid = challenge_playlists[\"pid\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pid=pid.to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solution.insert(0, \"pid\", pid) #[*range(2000000,2010000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solution#[998:1005]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write solution to output file\n",
    "\n",
    "Include only the data not any index or headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solution_csv = solution.to_csv(index=False,header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = open(challenge_solution, \"w\")\n",
    "n = text_file.write(challenge_header+solution_csv)\n",
    "text_file.close()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
