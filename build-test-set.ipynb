{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a test and training split for the mpd.\n",
    "\n",
    "Build a test data set from the mpd using the playlist distribution found in the official challenge set.\n",
    "\n",
    "This extracts 10k playlists from the mpd as a test set substitution for the original challenge set.  It saves the original mpd data files as a new training set with the test set removed. Keeping the structure of the original file set will simplify operation of codes that expect that input.\n",
    "\n",
    "The constructed splits will be named by a directory like mpd-split-<description> that contains the test-set.json and a data subdir with the mpd slices.\n",
    "    \n",
    "The challenge set will then need to be constructed from the test-set.json so that codes can processes a challenge set of withheld data. Additional downstream processing with rate results submitted against the split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import re\n",
    "import collections\n",
    "import os\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the mpd slice files\n",
    "\n",
    "Create one big data frame to make it simple to select the random samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "playlists = pd.DataFrame()\n",
    "tracks = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = True\n",
    "quick = True\n",
    "max_files_for_quick_processing = 20\n",
    "\n",
    "# random state\n",
    "seed = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_mpd(path):\n",
    "    global playlists, tracks;\n",
    "    \n",
    "    count = 0\n",
    "    filenames = os.listdir(path)\n",
    "    for filename in sorted(filenames):\n",
    "        if filename.startswith(\"mpd.slice.\") and filename.endswith(\".json\"):\n",
    "            fullpath = os.sep.join((path, filename))\n",
    "            f = open(fullpath)\n",
    "            js = f.read()\n",
    "            f.close()\n",
    "            if debug: print(\"loaded {}:\".format(fullpath))\n",
    "            mpd_slice = json.loads(js)\n",
    "            # Flatten data\n",
    "            # extract slice info to keep association with original training files.\n",
    "            slice_info = mpd_slice['info']['slice']\n",
    "            slice_playlists = pd.json_normalize(mpd_slice, record_path=['playlists'])\n",
    "            slice_playlists[\"slice\"] = slice_info\n",
    "            if debug: print(\"slice length {}:\".format(len(slice_playlists)))\n",
    "            slice_tracks = pd.json_normalize(mpd_slice['playlists'], record_path=['tracks'], meta=['pid'])\n",
    "            # drop tracks from playlist dataframe\n",
    "            # not worth it to save space, just makes it harder to reconstruct the playlist\n",
    "            #slice_playlists.drop(columns='tracks', inplace=True)\n",
    "            playlists = playlists.append(slice_playlists)\n",
    "            tracks = tracks.append(slice_tracks)\n",
    "            count += 1\n",
    "\n",
    "            if quick and count > max_files_for_quick_processing:\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "process_mpd(\"data/mpd/data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(playlists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Challenge set distribution\n",
    "\n",
    "Just read the data distribution from the challenge set file directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data using Python JSON module\n",
    "with open('data/challenge_set.json','r') as f:\n",
    "    data = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten data\n",
    "challenge_playlists = pd.json_normalize(data, record_path=['playlists'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge_playlists.drop(columns=['tracks'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge_playlists.num_tracks.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for length, count in challenge_playlists.num_tracks.value_counts().iteritems():\n",
    "    print(\"len {} count {}\".format(length,count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compare distributions between challenge and train data set\n",
    "\n",
    "Can see that the challenge set is similar but has some boosted representation at the higher and lower ends, likely to accomidate the 4x use of 25 and 100 length playlists. and 2x use of 0,1,5 seed.\n",
    "\n",
    "The spikes in the mpd might be due to the natural boundaries people see as playlist length at 50+ 100+ and 150+ or maybe there was some defacto limit imposed by spotify for a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "playlists.hist( column=\"num_tracks\", bins=240)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge_playlists.hist( column=\"num_tracks\", bins=240)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract test set from training based on challenge distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = pd.DataFrame()\n",
    "\n",
    "for length, count in challenge_playlists.num_tracks.value_counts().iteritems():\n",
    "    if debug: print(\"len {} count {}\".format(length,count))\n",
    "    # shrink count by 10% of available tracks if there aren't enough\n",
    "    # should only happen during dev when full data set not in use\n",
    "    num_avail = len(playlists[playlists.num_tracks==length])\n",
    "    if (num_avail < count):\n",
    "        newcount=num_avail - int(num_avail * .10)\n",
    "        print(\"WARNING: adjusted len {} count from {} to {}\".format(length, count, newcount))\n",
    "        count=newcount\n",
    "    testset=testset.append(playlists[playlists.num_tracks==length].sample(n=count, random_state=seed))\n",
    "    #if debug: print(\"len(testset): {}\".format(len(testset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset.hist( column=\"num_tracks\", bins=240)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save test set as json\n",
    "\n",
    "Need to pull out the playlists in the test set and build a challenge set. \n",
    "Need to remove the rows from the training set that are now for testing only.\n",
    "Then build the training set without the test set.\n",
    "Each file needs a header.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testname=\"ex1-from-21k-train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "today=datetime.datetime.now(datetime.timezone.utc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(today.isoformat())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.mkdir(\"data/\"+testname)\n",
    "except FileExistsError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the info header\n",
    "testset.drop(columns=['slice'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a custom info header\n",
    "fileinfo = '''{{\n",
    "    \"info\": {{\n",
    "       \"generated_on\": \"{}\", \n",
    "       \"slice\": \"{}\", \n",
    "       \"version\": \"v1\"\n",
    "    }},\\n'''.format(today.isoformat(), testname) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testjson = testset.to_json(orient=\"records\", indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add extra indent to the json so it fits into the final output\n",
    "testjson = re.sub('\\n', '\\n    ', testjson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/'+testname+'/testset.json','w') as f:\n",
    "    f.write(fileinfo + '    \"playlists\": ' + testjson)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "# ignore\n",
    "don't do it this way, leave the tracks in with the playlists and then there is no need to reconstruct the playlist.\n",
    "\n",
    "# add an empty tracks column\n",
    "testset[\"tracks\"] = \"\"\n",
    "\n",
    "for pid in testset.pid:\n",
    "    playlist_tracks = tracks[tracks.pid==pid]\n",
    "    playlist_tracks = playlist_tracks.drop(columns=['pid'])\n",
    "    testset.loc[testset.pid == pid, 'tracks'] = '\"tracks\": ' + playlist_tracks.to_json(orient='records', indent=2)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
